fit_file: fit_experimental

slurm: {'nodes': 2,
  'partition': "cpu-share",
  'cpus_per_task': 1,
  'mem_per_cpu': '2G',
  'ntasks': 80,
  }



data_path: /home/wmai/Perturb_conn/preprocessed_data_creamer/2025_creamer_lds
num_data_sets: 80 # number of data sets to include. randi data has 110 so this leaves 30 for cross validation
num_train_steps: 200 # number of steps of EM
dynamics_lags: 1 # allows the dynamical system to fit weights for multiple time steps in the past. Note that this increases memory and time required by a factor of at least lags^2
dynamics_input_lags: 45 # number of time points in the past that inputs can affect the latents
emissions_input_lags: 1 # same but for emissions (not used in the paper)
ridge_lambda: Null # allows for L2 regularization of the dynamics matrix (not used in the paper)
neuron_freq: 0.1 # removes neurons that are measured less than this fraction of the data set

fit_type: em
verbose: True
use_memmap: False # allows storing of the matricies on scratch space. Significantly reduces memory usage and increases time to run

# the calcium dynamics always look strange at the start of a recording, possibly due to the laser being turned on
# cut out the first minute to let the system equilibrate
start_index: 120

hold_out: 'worm'
held_out_data: [] # allows you to specify specific data sets to hold out

param_props:
  update: # which parameters are fixed or learned during fitting
    dynamics_weights: True
    dynamics_input_weights: True
    dynamics_cov: True
    emissions_weights: False
    emissions_input_weights: False
    emissions_cov: True

  shape:
    dynamics_weights: full # [synaptic, full] synaptic constrains to the connectome
    dynamics_input_weights: diag # [diag, full]
    dynamics_cov: diag # [diag, full]
    emissions_cov: diag # [diag, full]

permute_mask: False # permute the mask 
random_seed: 0
plot_figures: True
model_save_folder: trained_models



